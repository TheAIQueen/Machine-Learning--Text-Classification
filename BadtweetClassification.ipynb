{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "\n",
    "test_data = pd.read_csv('data/nlp-getting-started/test.csv')\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data[[\"id\", \"text\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026</td>\n",
       "      <td>@LasVegasLocally @VitalVegas They reined it in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>@realhotcullen I agree but I knew we'd be goin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7076</td>\n",
       "      <td>http://t.co/HFqlwo1kMy E-Mini SP 500: Earnings...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  target\n",
       "0   542  WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEA...       0\n",
       "1  2026  @LasVegasLocally @VitalVegas They reined it in...       0\n",
       "2  1138  @realhotcullen I agree but I knew we'd be goin...       0\n",
       "3  7076  http://t.co/HFqlwo1kMy E-Mini SP 500: Earnings...       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial class distribution:\n",
      " target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class counts:\n",
      " target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Size of the smaller class: 3271\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Append minority data to the original dataframe\n",
    "\n",
    "# Check initial class distribution\n",
    "print(\"Initial class distribution:\\n\", df['target'].value_counts())\n",
    "\n",
    "# Count the number of instances for each label\n",
    "class_counts = df['target'].value_counts()\n",
    "print(\"\\nClass counts:\\n\", class_counts)\n",
    "\n",
    "# Determine the size of the smaller class\n",
    "min_class_size = class_counts.min()\n",
    "print(\"\\nSize of the smaller class:\", min_class_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced subset distribution:\n",
      " target\n",
      "0    3271\n",
      "1    3271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "min_class_size = df['target'].value_counts().min()\n",
    "\n",
    "# Create a balanced subset\n",
    "balanced_subset = df.groupby('target').apply(lambda x: x.sample(min_class_size)).reset_index(drop=True)\n",
    "\n",
    "# Check the balanced subset distribution\n",
    "print(\"\\nBalanced subset distribution:\\n\", balanced_subset['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026</td>\n",
       "      <td>@LasVegasLocally @VitalVegas They reined it in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1138</td>\n",
       "      <td>@realhotcullen I agree but I knew we'd be goin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7076</td>\n",
       "      <td>http://t.co/HFqlwo1kMy E-Mini SP 500: Earnings...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4493</td>\n",
       "      <td>I'm in the shower and I went to go change the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6537</th>\n",
       "      <td>8704</td>\n",
       "      <td>4 equipment ego break upon dig your family int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538</th>\n",
       "      <td>7535</td>\n",
       "      <td>Refugio oil spill may have been costlier bigge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>10074</td>\n",
       "      <td>RT_America: RT RT_com: Eye of Super Typhoon So...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>5389</td>\n",
       "      <td>Found this cool photo not mine 1952 Dodge Wayn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6541</th>\n",
       "      <td>3826</td>\n",
       "      <td>@channelstv:That's why terrorism is not d war ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6542 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  target\n",
       "0       542  WWI WWII JAPANESE ARMY NAVY MILITARY JAPAN LEA...       0\n",
       "1      2026  @LasVegasLocally @VitalVegas They reined it in...       0\n",
       "2      1138  @realhotcullen I agree but I knew we'd be goin...       0\n",
       "3      7076  http://t.co/HFqlwo1kMy E-Mini SP 500: Earnings...       0\n",
       "4      4493  I'm in the shower and I went to go change the ...       0\n",
       "...     ...                                                ...     ...\n",
       "6537   8704  4 equipment ego break upon dig your family int...       1\n",
       "6538   7535  Refugio oil spill may have been costlier bigge...       1\n",
       "6539  10074  RT_America: RT RT_com: Eye of Super Typhoon So...       1\n",
       "6540   5389  Found this cool photo not mine 1952 Dodge Wayn...       1\n",
       "6541   3826  @channelstv:That's why terrorism is not d war ...       1\n",
       "\n",
       "[6542 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7896\n",
      "Confusion Matrix:\n",
      " [[147  21]\n",
      " [ 48 112]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       168\n",
      "           1       0.84      0.70      0.76       160\n",
      "\n",
      "    accuracy                           0.79       328\n",
      "   macro avg       0.80      0.79      0.79       328\n",
      "weighted avg       0.80      0.79      0.79       328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "df = balanced_subset[[\"id\", \"text\", \"target\"]]\n",
    "# Preprocessing\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])  # Use 'v2' for messages\n",
    "y = df['target']  # Use 'v1' for labels now containing 0 and 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = SVC(probability=True, random_state=42) # Increased max_iter for convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Ensure that the test dataset has the same preprocessing steps applied\n",
    "# Initialize the TF-IDF Vectorizer again to fit on the training set\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Preprocess the training data to fit the vectorizer\n",
    "df = train_data[[\"id\", \"text\", \"target\"]]\n",
    "X_train = vectorizer.fit_transform(df['text'])  # Fit the vectorizer on training text\n",
    "y_train = df['target']\n",
    "\n",
    "# Train the model as before\n",
    "model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test = vectorizer.transform(test_data['text'])  # Use the same vectorizer to transform the test text\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Prepare the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission_logreg.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7718\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.94      0.82      1748\n",
      "           1       0.86      0.55      0.67      1298\n",
      "\n",
      "    accuracy                           0.77      3046\n",
      "   macro avg       0.80      0.74      0.75      3046\n",
      "weighted avg       0.79      0.77      0.76      3046\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      " [[1635  113]\n",
      " [ 582  716]]\n",
      "------------------------------------------------------------\n",
      "Gradient Boosting Accuracy: 0.7489\n",
      "Classification Report for Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81      1748\n",
      "           1       0.86      0.49      0.63      1298\n",
      "\n",
      "    accuracy                           0.75      3046\n",
      "   macro avg       0.78      0.72      0.72      3046\n",
      "weighted avg       0.77      0.75      0.73      3046\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      " [[1640  108]\n",
      " [ 657  641]]\n",
      "------------------------------------------------------------\n",
      "Support Vector Machine Accuracy: 0.7909\n",
      "Classification Report for Support Vector Machine:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.84      1748\n",
      "           1       0.88      0.59      0.70      1298\n",
      "\n",
      "    accuracy                           0.79      3046\n",
      "   macro avg       0.82      0.76      0.77      3046\n",
      "weighted avg       0.81      0.79      0.78      3046\n",
      "\n",
      "Confusion Matrix for Support Vector Machine:\n",
      " [[1648  100]\n",
      " [ 537  761]]\n",
      "------------------------------------------------------------\n",
      "Multinomial Naive Bayes Accuracy: 0.7912\n",
      "Classification Report for Multinomial Naive Bayes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83      1748\n",
      "           1       0.84      0.63      0.72      1298\n",
      "\n",
      "    accuracy                           0.79      3046\n",
      "   macro avg       0.80      0.77      0.78      3046\n",
      "weighted avg       0.80      0.79      0.79      3046\n",
      "\n",
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      " [[1588  160]\n",
      " [ 476  822]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [19:31:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7738\n",
      "Classification Report for XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82      1748\n",
      "           1       0.81      0.61      0.70      1298\n",
      "\n",
      "    accuracy                           0.77      3046\n",
      "   macro avg       0.78      0.75      0.76      3046\n",
      "weighted avg       0.78      0.77      0.77      3046\n",
      "\n",
      "Confusion Matrix for XGBoost:\n",
      " [[1564  184]\n",
      " [ 505  793]]\n",
      "------------------------------------------------------------\n",
      "[LightGBM] [Info] Number of positive: 1973, number of negative: 2594\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4565\n",
      "[LightGBM] [Info] Number of data points in the train set: 4567, number of used features: 353\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432012 -> initscore=-0.273646\n",
      "[LightGBM] [Info] Start training from score -0.273646\n",
      "LightGBM Accuracy: 0.7531\n",
      "Classification Report for LightGBM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.86      0.80      1748\n",
      "           1       0.76      0.61      0.68      1298\n",
      "\n",
      "    accuracy                           0.75      3046\n",
      "   macro avg       0.76      0.73      0.74      3046\n",
      "weighted avg       0.75      0.75      0.75      3046\n",
      "\n",
      "Confusion Matrix for LightGBM:\n",
      " [[1505  243]\n",
      " [ 509  789]]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = train_data[[\"id\", \"text\", \"target\"]]\n",
    "\n",
    "# Preprocess the data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])  # Fit on training text\n",
    "y = df['target']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(),\n",
    "}\n",
    "\n",
    "# Train each model and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'{name} Accuracy: {accuracy:.4f}')\n",
    "    print(f'Classification Report for {name}:\\n', classification_report(y_val, y_pred))\n",
    "    print(f'Confusion Matrix for {name}:\\n', confusion_matrix(y_val, y_pred))\n",
    "    print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7735\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       874\n",
      "           1       0.83      0.58      0.69       649\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.79      0.75      0.75      1523\n",
      "weighted avg       0.78      0.77      0.76      1523\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      " [[799  75]\n",
      " [270 379]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_random_forest.csv' created successfully.\n",
      "Gradient Boosting Accuracy: 0.7551\n",
      "Classification Report for Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81       874\n",
      "           1       0.85      0.51      0.64       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.79      0.72      0.73      1523\n",
      "weighted avg       0.78      0.76      0.74      1523\n",
      "\n",
      "Confusion Matrix for Gradient Boosting:\n",
      " [[816  58]\n",
      " [315 334]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_gradient_boosting.csv' created successfully.\n",
      "Support Vector Machine Accuracy: 0.7912\n",
      "Classification Report for Support Vector Machine:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.84       874\n",
      "           1       0.85      0.61      0.72       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.81      0.77      0.78      1523\n",
      "weighted avg       0.80      0.79      0.78      1523\n",
      "\n",
      "Confusion Matrix for Support Vector Machine:\n",
      " [[806  68]\n",
      " [250 399]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_support_vector_machine.csv' created successfully.\n",
      "Multinomial Naive Bayes Accuracy: 0.7932\n",
      "Classification Report for Multinomial Naive Bayes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83       874\n",
      "           1       0.83      0.65      0.73       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.77      0.78      1523\n",
      "weighted avg       0.80      0.79      0.79      1523\n",
      "\n",
      "Confusion Matrix for Multinomial Naive Bayes:\n",
      " [[786  88]\n",
      " [227 422]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_multinomial_naive_bayes.csv' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [19:36:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7649\n",
      "Classification Report for XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       874\n",
      "           1       0.79      0.61      0.69       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.77      0.75      0.75      1523\n",
      "weighted avg       0.77      0.76      0.76      1523\n",
      "\n",
      "Confusion Matrix for XGBoost:\n",
      " [[768 106]\n",
      " [252 397]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_xgboost.csv' created successfully.\n",
      "[LightGBM] [Info] Number of positive: 2622, number of negative: 3468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7341\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 551\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.430542 -> initscore=-0.279641\n",
      "[LightGBM] [Info] Start training from score -0.279641\n",
      "LightGBM Accuracy: 0.7603\n",
      "Classification Report for LightGBM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.80       874\n",
      "           1       0.77      0.63      0.69       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.76      0.74      0.75      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n",
      "Confusion Matrix for LightGBM:\n",
      " [[750 124]\n",
      " [241 408]]\n",
      "------------------------------------------------------------\n",
      "Submission file 'submission_lightgbm.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the training dataset\n",
    "df = train_data[[\"id\", \"text\", \"target\"]]\n",
    "\n",
    "# Preprocess the data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['text'])  # Fit on training text\n",
    "y = df['target']  # Target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(),\n",
    "}\n",
    "\n",
    "# Load the test dataset\n",
    "X_test = vectorizer.transform(test_data['text'])  # Use the same vectorizer to transform the test text\n",
    "\n",
    "# Train each model, evaluate, and create submission files\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'{name} Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Print classification report and confusion matrix for validation set\n",
    "    print(f'Classification Report for {name}:\\n', classification_report(y_val, y_pred))\n",
    "    print(f'Confusion Matrix for {name}:\\n', confusion_matrix(y_val, y_pred))\n",
    "    print('-' * 60)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # Prepare the submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data['id'],\n",
    "        'target': test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the submission file with the model name\n",
    "    submission_file_name = f'submission_{name.replace(\" \", \"_\").lower()}.csv'\n",
    "    submission.to_csv(submission_file_name, index=False)\n",
    "    print(f\"Submission file '{submission_file_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/p98pc2xd7z743rkklxhjfp2w0000gn/T/ipykernel_5026/1868326456.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = df['target'].astype(int)  # Ensure target is of type int\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92671ae728b24efc9f2a033edaf666d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "\n",
    "test_data = pd.read_csv('data/nlp-getting-started/test.csv')\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(train_data.head())\n",
    "\n",
    "df = train_data[[\"id\", \"text\", \"target\"]]\n",
    "\n",
    "# Preprocess the data\n",
    "df['target'] = df['target'].astype(int)  # Ensure target is of type int\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create dataset objects\n",
    "class DisasterTweetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = DisasterTweetsDataset(train_encodings, y_train.to_numpy())\n",
    "val_dataset = DisasterTweetsDataset(val_encodings, y_val.to_numpy())\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(val_dataset)\n",
    "y_pred = torch.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:\\n', classification_report(y_val, y_pred))\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_data = pd.read_csv('test.csv')  # Adjust path as needed\n",
    "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = DisasterTweetsDataset(test_encodings, [0] * len(test_data))  # Dummy labels\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_pred_labels = torch.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'target': test_pred_labels.numpy()\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('submission_bert.csv', index=False)\n",
    "print(\"Submission file 'submission_bert.csv' created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
